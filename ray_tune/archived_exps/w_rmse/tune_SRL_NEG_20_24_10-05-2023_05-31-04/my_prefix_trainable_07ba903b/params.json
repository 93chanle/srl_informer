{
  "batch_size": 16,
  "d_layers": 1,
  "d_model": 256,
  "e_layers": 5,
  "label_seq_len_ratio": 0.525,
  "learning_rate": 0.0005308599762908329,
  "n_heads": 32,
  "seq_len": 56,
  "train_epochs": 10,
  "wrmse_weight": 1.0
}